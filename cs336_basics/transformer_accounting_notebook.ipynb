{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Accounting\n",
    "\n",
    "## Basic Memory Calculations\n",
    "Let $E$ be embedding mem, $L$ num layers\n",
    "$$ \n",
    "M = \\underbrace{|Vocab|d}_{E} + L\\left(\\underbrace{3d^2 + d^2}_{W_{QKV} + W_O} + \\underbrace{3d_{ff}d}_{FFN} + 2\\underbrace{d}_{\\text{LN}}\\right) + \\underbrace{d}_{\\text{LN}} + \\underbrace{|Vocab|d}_{\\text{Ouput}}\n",
    "$$\n",
    "\n",
    "\n",
    "## Basic FLOPS Calculations\n",
    "\n",
    "Transformer block dominates, focus there. Split into MHA and FFN. Let $c$ be context length, $h$ be number of heads\n",
    "$$\n",
    "MHA = \\underbrace{6cd^2}_{Q, K, V} + h\\left(\\underbrace{2c^2(d/h)}_{Q_hK_h^T} + \\underbrace{2c^2(d/h)}_{\\text{softmax}\\cdot V_h}\\right) + \\underbrace{2d^2c}_{W_O \\text{ mult}}\n",
    "$$\n",
    "All the multiplies take same amount of time.\n",
    "$$\n",
    "FFN = \\underbrace{6d_{ff} d c}_{W_2(\\sigma(W_1x)\\odot W_3x)}\n",
    "$$\n",
    "\n",
    "### Flops Conclusion\n",
    "\n",
    "When context lenght dominates, MHA is more expensive. When $d_{ff}$ dominates, the FFN is more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nb_trainable_params(param_dict):\n",
    "    # Embedding layer\n",
    "\n",
    "    vocab_size = param_dict['vocab_size']\n",
    "    num_layers = param_dict['num_layers']\n",
    "    d_model = param_dict['d_model']\n",
    "    d_ff = param_dict['d_ff']\n",
    "\n",
    "    nb_params = 0\n",
    "    embedding = d_model*vocab_size\n",
    "    nb_params += embedding\n",
    "    # Transformer Block layers\n",
    "    ## Feed forward layer\n",
    "    ffn_w1 = d_ff*d_model\n",
    "    ffn_w2 = d_ff*d_model\n",
    "    ffn_w3 = d_model * d_ff\n",
    "    nb_params += ffn_w1+ffn_w2+ ffn_w3\n",
    "\n",
    "    ## MHA params\n",
    "    W_QKV = 3*d_model**2\n",
    "    W_O = d_model**2\n",
    "    nb_params += W_QKV + W_O\n",
    "\n",
    "    ## Layer Norm \n",
    "    ln_in_block = 2*d_model\n",
    "    nb_params += ln_in_block\n",
    "\n",
    "    ## Multiplied by num_layers \n",
    "    nb_params *= num_layers\n",
    "\n",
    "    # final LN\n",
    "    ln_final = d_model\n",
    "    nb_params += ln_final\n",
    "\n",
    "    # final output layer\n",
    "    output = vocab_size * d_model\n",
    "    nb_params += output\n",
    "\n",
    "    return nb_params\n",
    "\n",
    "# flops focusing on costly parts\n",
    "def nb_flops(param_dict): \n",
    "    nb_flops = 0\n",
    "    nb_flops_by_part = {'MHA' : 0, 'FFN' : 0, 'ouput' : 0} \n",
    "    vocab_size = param_dict['vocab_size']\n",
    "    num_layers = param_dict['num_layers']\n",
    "    context_length = param_dict['context_length']\n",
    "    d_model = param_dict['d_model']\n",
    "    num_heads = param_dict['num_heads']\n",
    "    d_ff = param_dict['d_ff']\n",
    "    \n",
    "    # Transformer block layers\n",
    "    \n",
    "    ## MHA (ignoring softmax)\n",
    "    ### Project input to weights QKV\n",
    "    proj_flops = 2*(3*d_model**2)*(context_length)\n",
    "    ### Per heads QK^T\n",
    "    QKT_flops = 2*(context_length**2)*(d_model//num_heads)\n",
    "    ### Value flops\n",
    "    V_flops = 2*(context_length**2)*(d_model//num_heads)\n",
    "    ### W_O flops\n",
    "    W_O_flops  = 2*d_model**2 * context_length\n",
    "    nb_flops_by_part['MHA'] = proj_flops+ num_heads*(QKT_flops+V_flops)+W_O_flops\n",
    "    nb_flops_by_part['MHA'] *= num_layers\n",
    "    \n",
    "    ## FFN\n",
    "    ### Multiply input (ignoring nonlinearity and the odot) \n",
    "    W1x_flops = 2*d_ff*d_model*context_length\n",
    "    W3x_flops = W1x_flops\n",
    "    W2_sigma_W1x_dot_W3x = W1x_flops\n",
    "    nb_flops_by_part['FFN'] = W1x_flops + W3x_flops + W2_sigma_W1x_dot_W3x\n",
    "    nb_flops_by_part['FFN'] *= num_layers\n",
    "\n",
    "    ## Output layer\n",
    "    nb_flops_by_part['output'] = 2*vocab_size*d_model*context_length\n",
    "\n",
    "    for count in nb_flops_by_part.values():\n",
    "        nb_flops += count\n",
    "    return nb_flops, nb_flops_by_part\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2-XL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters 5906384000\n",
      "Memory consumed 23.625536 GBs\n",
      "Number of flops total 4.5133365248 Teraflops\n",
      "Number of MHA flops 1.3287555072 Teraflops\n",
      "Number of FFN flops 3.01989888 Teraflops\n",
      "Number of output flops 0.1646821376 Teraflops\n",
      "FFN/MHA 2.272727272727273\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 XL \n",
    "gpt_2_xl = {}\n",
    "gpt_2_xl['vocab_size'] = 50257\n",
    "gpt_2_xl['context_length'] = 1024\n",
    "gpt_2_xl['num_layers'] = 48\n",
    "gpt_2_xl['d_model'] = 1600\n",
    "gpt_2_xl['num_heads'] = 25\n",
    "gpt_2_xl['d_ff'] = 6400\n",
    "\n",
    "nb_trainable = nb_trainable_params(gpt_2_xl)\n",
    "print(\"Number of trainable parameters\", nb_trainable)\n",
    "\n",
    "print(\"Memory consumed\", nb_trainable*4 /(10**9), \"GBs\")\n",
    "\n",
    "nb_flops_total, nb_flops_by_part = nb_flops(gpt_2_xl)\n",
    "print(\"Number of flops total\", nb_flops_total/(10**12), \"Teraflops\")\n",
    "print(\"Number of MHA flops\", nb_flops_by_part['MHA']/(10**12), \"Teraflops\")\n",
    "print(\"Number of FFN flops\", nb_flops_by_part['FFN']/(10**12), \"Teraflops\")\n",
    "print(\"Number of output flops\", nb_flops_by_part['output']/(10**12), \"Teraflops\")\n",
    "\n",
    "print(\"FFN/MHA\", nb_flops_by_part['FFN']/nb_flops_by_part['MHA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters 615031296\n",
      "Memory consumed 2.460125184 GBs\n",
      "Number of flops total 0.349630365696 Teraflops\n",
      "Number of MHA flops 0.09663676416 Teraflops\n",
      "Number of FFN flops 0.173946175488 Teraflops\n",
      "Number of output flops 0.079047426048 Teraflops\n",
      "FFN/MHA 1.8\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 XL \n",
    "gpt_2_small = {}\n",
    "gpt_2_small['vocab_size'] = 50257\n",
    "gpt_2_small['context_length'] = 1024\n",
    "gpt_2_small['num_layers'] = 12\n",
    "gpt_2_small['d_model'] = 768\n",
    "gpt_2_small['num_heads'] = 12\n",
    "gpt_2_small['d_ff'] = 3072\n",
    "\n",
    "nb_trainable = nb_trainable_params(gpt_2_small)\n",
    "print(\"Number of trainable parameters\", nb_trainable)\n",
    "\n",
    "print(\"Memory consumed\", nb_trainable*4 /(10**9), \"GBs\")\n",
    "\n",
    "nb_flops_total, nb_flops_by_part = nb_flops(gpt_2_small)\n",
    "print(\"Number of flops total\", nb_flops_total/(10**12), \"Teraflops\")\n",
    "print(\"Number of MHA flops\", nb_flops_by_part['MHA']/(10**12), \"Teraflops\")\n",
    "print(\"Number of FFN flops\", nb_flops_by_part['FFN']/(10**12), \"Teraflops\")\n",
    "print(\"Number of output flops\", nb_flops_by_part['output']/(10**12), \"Teraflops\")\n",
    "\n",
    "print(\"FFN/MHA\", nb_flops_by_part['FFN']/nb_flops_by_part['MHA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT-2 Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters 1764780032\n",
      "Memory consumed 7.059120128 GBs\n",
      "Number of flops total 1.187728326656 Teraflops\n",
      "Number of MHA flops 0.309237645312 Teraflops\n",
      "Number of FFN flops 0.77309411328 Teraflops\n",
      "Number of output flops 0.105396568064 Teraflops\n",
      "FFN/MHA 2.5\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 XL \n",
    "gpt_2_large = {}\n",
    "gpt_2_large['vocab_size'] = 50257\n",
    "gpt_2_large['context_length'] = 1024\n",
    "gpt_2_large['num_layers'] = 24\n",
    "gpt_2_large['d_model'] = 1024\n",
    "gpt_2_large['num_heads'] = 16\n",
    "gpt_2_large['d_ff'] = 5120\n",
    "\n",
    "nb_trainable = nb_trainable_params(gpt_2_large)\n",
    "print(\"Number of trainable parameters\", nb_trainable)\n",
    "\n",
    "print(\"Memory consumed\", nb_trainable*4 /(10**9), \"GBs\")\n",
    "\n",
    "nb_flops_total, nb_flops_by_part = nb_flops(gpt_2_large)\n",
    "print(\"Number of flops total\", nb_flops_total/(10**12), \"Teraflops\")\n",
    "print(\"Number of MHA flops\", nb_flops_by_part['MHA']/(10**12), \"Teraflops\")\n",
    "print(\"Number of FFN flops\", nb_flops_by_part['FFN']/(10**12), \"Teraflops\")\n",
    "print(\"Number of output flops\", nb_flops_by_part['output']/(10**12), \"Teraflops\")\n",
    "\n",
    "print(\"FFN/MHA\", nb_flops_by_part['FFN']/nb_flops_by_part['MHA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT-XL long context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters 5906384000\n",
      "Memory consumed 23.625536 GBs\n",
      "Number of flops total 149.5227957248 Teraflops\n",
      "Number of MHA flops 98.5694994432 Teraflops\n",
      "Number of FFN flops 48.31838208 Teraflops\n",
      "Number of output flops 2.6349142016 Teraflops\n",
      "FFN/MHA 0.49019607843137253\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 XL \n",
    "gpt_2_xl = {}\n",
    "gpt_2_xl['vocab_size'] = 50257\n",
    "gpt_2_xl['context_length'] = 16384\n",
    "gpt_2_xl['num_layers'] = 48\n",
    "gpt_2_xl['d_model'] = 1600\n",
    "gpt_2_xl['num_heads'] = 25\n",
    "gpt_2_xl['d_ff'] = 6400\n",
    "\n",
    "nb_trainable = nb_trainable_params(gpt_2_xl)\n",
    "print(\"Number of trainable parameters\", nb_trainable)\n",
    "\n",
    "print(\"Memory consumed\", nb_trainable*4 /(10**9), \"GBs\")\n",
    "\n",
    "nb_flops_total, nb_flops_by_part = nb_flops(gpt_2_xl)\n",
    "print(\"Number of flops total\", nb_flops_total/(10**12), \"Teraflops\")\n",
    "print(\"Number of MHA flops\", nb_flops_by_part['MHA']/(10**12), \"Teraflops\")\n",
    "print(\"Number of FFN flops\", nb_flops_by_part['FFN']/(10**12), \"Teraflops\")\n",
    "print(\"Number of output flops\", nb_flops_by_part['output']/(10**12), \"Teraflops\")\n",
    "\n",
    "print(\"FFN/MHA\", nb_flops_by_part['FFN']/nb_flops_by_part['MHA'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
